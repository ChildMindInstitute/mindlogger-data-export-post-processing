{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os, sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to extract score from options based on values in the response\n",
    "def val_score_mapping(s1,s2):\n",
    "    \n",
    "    split_options = s2.strip().split(\"),\")\n",
    "    split_response = s1.strip().split(\": \")[1].split(',')\n",
    "    scores = {}\n",
    "\n",
    "    for i in split_options:\n",
    "        if len(i) > 0:\n",
    "            val_num = i.split(\"(score\")[0].split(': ')[1].strip()\n",
    "            score_num = i.split(\"(score\")[1].split(': ')[1].strip()\n",
    "            scores[val_num] = score_num\n",
    "\n",
    "    response_score_mapping = {split_response[i].strip(): scores[split_response[i].strip()] for i in range(len(split_response))}\n",
    "    list_response_score_mapping = list(response_score_mapping.values())\n",
    "    str_response_score_mapping = ', '.join(str(value) for value in list_response_score_mapping)\n",
    "    return str_response_score_mapping.replace(')', '')\n",
    "\n",
    "\n",
    "# Function to cleanup and split time range in the response\n",
    "def clean_time_range(df, column_name):\n",
    "    cleaned = [] \n",
    "    for i in range(len(df[column_name])):\n",
    "        if pd.notna(df[column_name][i]) and str(df[column_name][i]).startswith('time_range'):\n",
    "            t = re.sub(r'[a-zA-Z\\s+(\\)_:]', '', df[column_name][i])\n",
    "            t = t.replace(',', ':')\n",
    "            if re.search(r'^[0-9]:', t): #9,30/12,30\n",
    "                ttemp = '0' + t\n",
    "            elif re.search(r':[0-9]$', t): #12,5/12,30\n",
    "                ttemp = t.replace(':', ':0')\n",
    "            else:\n",
    "                ttemp = t\n",
    "            # tpos = datetime.strptime(str(ttemp), '%H:%M')\n",
    "            # thm = tpos.strftime('%H:%M')\n",
    "            thm = ttemp\n",
    "        else:\n",
    "            ttemp = df[column_name][i] \n",
    "            thm = ttemp\n",
    "        cleaned.append(thm)\n",
    "    return cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup path variable and get list of report.csv files\n",
    "\n",
    "input_path = os.path.expanduser('~/NIMH EMA Data v2/Input Files/')\n",
    "all_files = os.listdir(os.path.join(input_path, 'EMA_applet_data'))\n",
    "files = [file for file in all_files if file.startswith('report')]\n",
    "input_files = os.listdir(input_path)\n",
    "output_path = os.path.expanduser('~/NIMH EMA Data v2/Output Files/')\n",
    "\n",
    "#Read all the report.csv files\n",
    "report_all = []\n",
    "for i in range(len(files)):\n",
    "    temp_df = pd.read_csv(os.path.join(input_path, 'EMA_applet_data', files[i]), encoding='ISO-8859-1')\n",
    "    report_all.append(temp_df)\n",
    "\n",
    "# Concat report.csv to one file and read other input files\n",
    "dat_full = pd.concat(report_all, ignore_index=True)\n",
    "flow = pd.read_csv(os.path.join(input_path, 'flow-items.csv'))\n",
    "history = pd.read_csv(os.path.join(input_path, 'user-flow-schedule.csv'))\n",
    "if 'user-activity-schedule.csv' in input_files:\n",
    "    act_history_exist = 1\n",
    "    act_history = pd.read_csv(os.path.join(input_path, 'user-activity-schedule.csv'))\n",
    "else: \n",
    "    act_history_exist = 0\n",
    "dat_full = dat_full.applymap(str)\n",
    "\n",
    "# Write out the concatenated report.csv file\n",
    "dat_full.to_csv(os.path.join(output_path,'report_all.csv'),index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename id column as it contains special characters\n",
    "dat_full.rename(columns={dat_full.columns[0]: 'id'}, inplace=True) \n",
    "\n",
    "# Add timezone_offset\n",
    "dat_full['offset'] = np.where(dat_full['timezone_offset']=='nan', 0, 1)\n",
    "\n",
    "dat_full['activity_start_time_offsetADD'] = np.where(dat_full['timezone_offset'] != 'nan', pd.to_numeric(dat_full['activity_start_time'], errors='coerce')+ (pd.to_numeric(dat_full['timezone_offset'], errors='coerce')* 60 * 1000) , pd.to_numeric(dat_full['activity_start_time'], errors='coerce'))\n",
    "dat_full['activity_end_time_offsetADD'] = np.where(dat_full['timezone_offset'] != 'nan', pd.to_numeric(dat_full['activity_end_time'], errors='coerce')+ (pd.to_numeric(dat_full['timezone_offset'], errors='coerce')* 60 * 1000) , pd.to_numeric(dat_full['activity_end_time'], errors='coerce'))\n",
    "dat_full['activity_scheduled_time_offsetADD'] = np.where(dat_full['timezone_offset'] != 'nan', pd.to_numeric(dat_full['activity_scheduled_time'], errors='coerce')+(pd.to_numeric(dat_full['timezone_offset'], errors='coerce')* 60 * 1000) , pd.to_numeric(dat_full['activity_scheduled_time'], errors='coerce'))\n",
    "\n",
    "dat_full['activity_start_time_offsetADD'] = pd.to_numeric(dat_full['activity_start_time_offsetADD'], downcast='integer')\n",
    "dat_full['activity_end_time_offsetADD'] = pd.to_numeric(dat_full['activity_end_time_offsetADD'], downcast='integer')\n",
    "dat_full['activity_scheduled_time_offsetADD'] = pd.to_numeric(dat_full['activity_scheduled_time_offsetADD'], downcast='integer')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### R code translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting from here is the translation of R code\n",
    "dat_full['start_Time'] = dat_full['activity_start_time_offsetADD']\n",
    "dat_full['end_Time'] = dat_full['activity_end_time_offsetADD']\n",
    "dat_full['schedule_Time'] = dat_full['activity_scheduled_time_offsetADD']\n",
    "\n",
    "# Cleanup similar to R code\n",
    "dat_processed = dat_full.groupby(['secret_user_id', 'activity_flow_id', 'activity_scheduled_time'], group_keys=True).apply(lambda x: x.assign(start_Time=x['start_Time'].min(), end_Time=x['end_Time'].max())).reset_index(drop=True)\n",
    "\n",
    "# added extra columns than what exists in R cleanup code\n",
    "dat_subset = dat_processed[['id', 'activity_scheduled_time', 'secret_user_id', 'userId',\n",
    "       'activity_id', 'activity_name', 'activity_flow_id', 'activity_flow_name', 'item', 'response', 'options',\n",
    "       'event_id', 'start_Time', 'end_Time', 'schedule_Time', 'version', 'activity_start_time', 'offset']]\n",
    "\n",
    "# Creating additional column to add scores in the next steps\n",
    "dat_subset = dat_subset.copy()\n",
    "dat_subset['response_scores'] = None\n",
    "\n",
    "# Cleanup similar to R code\n",
    "for i in range(len(dat_subset['response'])):\n",
    "    if re.search(r'score: ', dat_subset['options'][i]):\n",
    "        s = val_score_mapping(dat_subset['response'][i], dat_subset['options'][i])\n",
    "        dat_subset.loc[i, 'response_scores'] = s  \n",
    "    if re.search(r'value', dat_subset['response'][i]):\n",
    "        r = dat_subset['response'][i].replace(\"value: \", \"\")\n",
    "        dat_subset.loc[i, 'response'] = r\n",
    "    elif re.search(r'time:', dat_subset['response'][i]):\n",
    "        if re.search(r'hr [0-9],', dat_subset['response'][i]): \n",
    "            egapp = dat_subset['response'][i]. replace('time: hr ', '0')\n",
    "            if re.search(r', min [0-9]$', egapp): \n",
    "                egtemp = egapp.replace(', min ', ':0')\n",
    "            elif re.search(r', min [0-9][0-9]$', egapp): \n",
    "                egtemp = egapp.replace(', min ', ':')\n",
    "            egpos = datetime.strptime(egtemp, '%H:%M')\n",
    "            egpos2 = egpos.strftime('%H:%M')\n",
    "            dat_subset.loc[i, 'response'] = egpos2\n",
    "        elif re.search(r'hr [0-9][0-9],', dat_subset['response'][i]):\n",
    "            egapp = dat_subset['response'][i].replace('time: hr ', '')\n",
    "            if re.search(r', min [0-9]$', egapp): \n",
    "                egtemp = egapp.replace(', min ', ':0')\n",
    "            elif re.search(r', min [0-9][0-9]$', egapp): \n",
    "                egtemp = egapp.replace(', min ', ':')\n",
    "            egpos = datetime.strptime(egtemp, '%H:%M')\n",
    "            egpos2 = egpos.strftime('%H:%M')\n",
    "            dat_subset.loc[i, 'response'] = egpos2\n",
    "    elif re.search(r'geo:', dat_subset['response'][i]):\n",
    "        g = dat_subset['response'][i].replace('geo: ', '')\n",
    "        dat_subset.loc[i, 'response'] = g\n",
    "\n",
    "# Combining scores and other formats of responses into one column\n",
    "dat_subset['response2'] = np.where(dat_subset['response_scores'].isna(), dat_subset['response'], dat_subset['response_scores'])\n",
    "\n",
    "# Sorting and Selecting required columns\n",
    "dat_subset = dat_subset.sort_values(by=['secret_user_id', 'activity_flow_id', 'activity_id', 'schedule_Time', 'activity_start_time'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new binary to indicate whether responses were from activity or flow\n",
    "dat_subset['is_activity'] = np.where(dat_subset['activity_flow_id'] == 'nan', 'Y', 'N')\n",
    "\n",
    "# Creating a new column such that:\n",
    "#   activity_flow_id if item is from flow \n",
    "#   activity_id if them is from activity\n",
    "dat_subset['activity_flow'] = np.where(dat_subset['activity_flow_id'] == 'nan', (dat_subset['activity_id'] + '|' + dat_subset['activity_name']), dat_subset['activity_flow_id'])\n",
    "dat_subset = dat_subset[['userId', 'secret_user_id', 'activity_flow_id', 'activity_id', 'activity_flow', 'activity_flow_name', 'is_activity', 'offset', 'item', 'response', \n",
    "                         'response_scores', 'response2','options', 'start_Time',\n",
    "                         'end_Time', 'schedule_Time', 'activity_scheduled_time', 'version', 'id', 'event_id']]\n",
    "\n",
    "dat_subset = dat_subset.rename(columns={'event_id': 'event_id_report'})\n",
    "\n",
    "# Making sure there are no NAs \n",
    "dat_subset['schedule_Time'] = np.where(dat_subset['schedule_Time'].isna(), 'NO SCHEDULE', dat_subset['schedule_Time'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Widening Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>secret_user_id</th>\n",
       "      <th>activity_flow</th>\n",
       "      <th>activity_flow_name</th>\n",
       "      <th>event_id_report</th>\n",
       "      <th>is_activity</th>\n",
       "      <th>start_Time</th>\n",
       "      <th>end_Time</th>\n",
       "      <th>schedule_Time</th>\n",
       "      <th>offset</th>\n",
       "      <th>...</th>\n",
       "      <th>socialmedia_activity</th>\n",
       "      <th>socialmedia_duration</th>\n",
       "      <th>strangers_communication_method</th>\n",
       "      <th>substances_lastnight</th>\n",
       "      <th>substances_lastnight_time</th>\n",
       "      <th>substances_thismorning_time</th>\n",
       "      <th>substances_tobacco</th>\n",
       "      <th>substances_tobacco_crave</th>\n",
       "      <th>videogame_duration</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>08077b30-b58e-40ad-83f7-9e1ace788673</td>\n",
       "      <td>sj_kt01</td>\n",
       "      <td>283c802b-aaed-41d1-8ab2-08ae87273510</td>\n",
       "      <td>Evening Assessment</td>\n",
       "      <td>52dc05e4-e0bf-45f2-802c-e3c568a07782</td>\n",
       "      <td>N</td>\n",
       "      <td>1707833143490</td>\n",
       "      <td>1718244072303</td>\n",
       "      <td>NO SCHEDULE</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a544d5da-ad21-4e5a-94b9-3030e4584fbb|a544d5da-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>08077b30-b58e-40ad-83f7-9e1ace788673</td>\n",
       "      <td>sj_kt01</td>\n",
       "      <td>283c802b-aaed-41d1-8ab2-08ae87273510</td>\n",
       "      <td>Evening Assessment</td>\n",
       "      <td>ab3d670d-74e2-48c4-830f-c73c1ecc3591</td>\n",
       "      <td>N</td>\n",
       "      <td>1707833143490</td>\n",
       "      <td>1718244072303</td>\n",
       "      <td>NO SCHEDULE</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>c8ef8872-c001-4cb7-983a-7ae78ce45ee0|c8ef8872-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>08077b30-b58e-40ad-83f7-9e1ace788673</td>\n",
       "      <td>sj_kt01</td>\n",
       "      <td>283c802b-aaed-41d1-8ab2-08ae87273510</td>\n",
       "      <td>Evening Assessment</td>\n",
       "      <td>ab3d670d-74e2-48c4-830f-c73c1ecc3591</td>\n",
       "      <td>N</td>\n",
       "      <td>1707833143490</td>\n",
       "      <td>1718244072303</td>\n",
       "      <td>NO SCHEDULE</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>aef9fefb-c9c1-4dd0-beb9-a91799103e3b|aef9fefb-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>08077b30-b58e-40ad-83f7-9e1ace788673</td>\n",
       "      <td>sj_kt01</td>\n",
       "      <td>283c802b-aaed-41d1-8ab2-08ae87273510</td>\n",
       "      <td>Evening Assessment</td>\n",
       "      <td>ab3d670d-74e2-48c4-830f-c73c1ecc3591</td>\n",
       "      <td>N</td>\n",
       "      <td>1707833143490</td>\n",
       "      <td>1718244072303</td>\n",
       "      <td>NO SCHEDULE</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>608e9ff2-0175-42bb-b689-f65dcdfaaf3f|608e9ff2-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>08077b30-b58e-40ad-83f7-9e1ace788673</td>\n",
       "      <td>sj_kt01</td>\n",
       "      <td>283c802b-aaed-41d1-8ab2-08ae87273510</td>\n",
       "      <td>Evening Assessment</td>\n",
       "      <td>ab3d670d-74e2-48c4-830f-c73c1ecc3591</td>\n",
       "      <td>N</td>\n",
       "      <td>1707833143490</td>\n",
       "      <td>1718244072303</td>\n",
       "      <td>NO SCHEDULE</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>a6e93890-fe22-4f3a-beb6-cedbe0b4df0f|a6e93890-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 userId secret_user_id  \\\n",
       "0  08077b30-b58e-40ad-83f7-9e1ace788673        sj_kt01   \n",
       "1  08077b30-b58e-40ad-83f7-9e1ace788673        sj_kt01   \n",
       "2  08077b30-b58e-40ad-83f7-9e1ace788673        sj_kt01   \n",
       "3  08077b30-b58e-40ad-83f7-9e1ace788673        sj_kt01   \n",
       "4  08077b30-b58e-40ad-83f7-9e1ace788673        sj_kt01   \n",
       "\n",
       "                          activity_flow  activity_flow_name  \\\n",
       "0  283c802b-aaed-41d1-8ab2-08ae87273510  Evening Assessment   \n",
       "1  283c802b-aaed-41d1-8ab2-08ae87273510  Evening Assessment   \n",
       "2  283c802b-aaed-41d1-8ab2-08ae87273510  Evening Assessment   \n",
       "3  283c802b-aaed-41d1-8ab2-08ae87273510  Evening Assessment   \n",
       "4  283c802b-aaed-41d1-8ab2-08ae87273510  Evening Assessment   \n",
       "\n",
       "                        event_id_report is_activity     start_Time  \\\n",
       "0  52dc05e4-e0bf-45f2-802c-e3c568a07782           N  1707833143490   \n",
       "1  ab3d670d-74e2-48c4-830f-c73c1ecc3591           N  1707833143490   \n",
       "2  ab3d670d-74e2-48c4-830f-c73c1ecc3591           N  1707833143490   \n",
       "3  ab3d670d-74e2-48c4-830f-c73c1ecc3591           N  1707833143490   \n",
       "4  ab3d670d-74e2-48c4-830f-c73c1ecc3591           N  1707833143490   \n",
       "\n",
       "        end_Time schedule_Time  offset  ... socialmedia_activity  \\\n",
       "0  1718244072303   NO SCHEDULE       0  ...                  NaN   \n",
       "1  1718244072303   NO SCHEDULE       0  ...                  NaN   \n",
       "2  1718244072303   NO SCHEDULE       0  ...                  NaN   \n",
       "3  1718244072303   NO SCHEDULE       0  ...                  NaN   \n",
       "4  1718244072303   NO SCHEDULE       1  ...                  NaN   \n",
       "\n",
       "  socialmedia_duration strangers_communication_method substances_lastnight  \\\n",
       "0                  NaN                            NaN                  NaN   \n",
       "1                  NaN                            NaN                  NaN   \n",
       "2                  NaN                            NaN                  NaN   \n",
       "3                  NaN                            NaN                  NaN   \n",
       "4                  NaN                            NaN                  NaN   \n",
       "\n",
       "  substances_lastnight_time substances_thismorning_time substances_tobacco  \\\n",
       "0                       NaN                         NaN                NaN   \n",
       "1                       NaN                         NaN                NaN   \n",
       "2                       NaN                         NaN                NaN   \n",
       "3                       NaN                         NaN                NaN   \n",
       "4                       NaN                         NaN                NaN   \n",
       "\n",
       "  substances_tobacco_crave videogame_duration  \\\n",
       "0                      NaN                NaN   \n",
       "1                      NaN                NaN   \n",
       "2                      NaN                NaN   \n",
       "3                      NaN                NaN   \n",
       "4                      NaN                  1   \n",
       "\n",
       "                                                  id  \n",
       "0  a544d5da-ad21-4e5a-94b9-3030e4584fbb|a544d5da-...  \n",
       "1  c8ef8872-c001-4cb7-983a-7ae78ce45ee0|c8ef8872-...  \n",
       "2  aef9fefb-c9c1-4dd0-beb9-a91799103e3b|aef9fefb-...  \n",
       "3  608e9ff2-0175-42bb-b689-f65dcdfaaf3f|608e9ff2-...  \n",
       "4  a6e93890-fe22-4f3a-beb6-cedbe0b4df0f|a6e93890-...  \n",
       "\n",
       "[5 rows x 204 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding answer_ids to help with debugging in the final output\n",
    "answers = dat_subset.groupby(['userId', 'secret_user_id', 'activity_flow', 'activity_flow_name', 'event_id_report', 'is_activity', 'start_Time', 'end_Time', 'schedule_Time', 'offset', 'version'])['id'].apply(lambda x: '|'.join(x.astype(str))).reset_index()\n",
    "\n",
    "# Widening data\n",
    "dat_wide = pd.pivot_table(dat_subset, index=['userId', 'secret_user_id', 'activity_flow', 'activity_flow_name', 'event_id_report', 'is_activity', 'start_Time', 'end_Time', 'schedule_Time', 'offset', 'version'], columns='item', values='response2', aggfunc='last').reset_index()\n",
    "\n",
    "# Joining Wide format table with answers to include concatinated answer_ids\n",
    "dat_wide = pd.merge(dat_wide, answers, on=['userId', 'secret_user_id', 'activity_flow', 'activity_flow_name', 'event_id_report', 'is_activity', 'start_Time', 'end_Time', 'schedule_Time', 'offset', 'version'], how='outer')\n",
    "\n",
    "dat_wide.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific item cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the function from the Set Up section to cleanup time range and split start and end dates from the response\n",
    "\n",
    "# Breaking up the old headache_time range question into 2\n",
    "dat_wide[['headache_time_start_old', 'headache_time_end_old']] = dat_wide['headache_time'].str.split(\"/\", expand=True)\n",
    "dat_wide['headache_time_start_old'] = clean_time_range(dat_wide, 'headache_time_start_old')\n",
    "dat_wide['headache_time_end_old'] = clean_time_range(dat_wide, 'headache_time_end_old')\n",
    "\n",
    "\n",
    "# Cleanup gps response and split to 2 columns for lat and lon\n",
    "dat_wide['now_gps'] = dat_wide['now_gps'].replace(r'[a-zA-Z\\s+(\\)]', '', regex = True)\n",
    "dat_wide[['now_gps_lat', 'now_gps_long']] = dat_wide['now_gps'].str.split(\"/\", expand=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Time Range Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of time range columns that need to be  split and formatted\n",
    "# PLEASE ADD MORE TO THIS LIST IF NECESSARY \n",
    "time_range_split = ['since_activity_monitor_time1', 'since_activity_monitor_time', 'since_light_device_time1', 'since_light_device_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in time_range_split: \n",
    "    start = i + '_start'\n",
    "    end = i + '_end'\n",
    "    dat_wide[[start, end]] = dat_wide[i].str.split(\"/\", expand=True)\n",
    "    dat_wide[start] = clean_time_range(dat_wide, start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup similar to R code\n",
    "dat_wide_full = dat_wide.applymap(str)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamp cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sailaja.yenepalli/anaconda3/lib/python3.11/site-packages/pandas/core/tools/datetimes.py:557: RuntimeWarning: invalid value encountered in cast\n",
      "  arr, tz_parsed = tslib.array_with_unit_to_datetime(arg, unit, errors=errors)\n"
     ]
    }
   ],
   "source": [
    "# Epoch to Timestamp\n",
    "dat_wide_full['start_Time'] = pd.to_numeric(dat_wide_full['start_Time'], errors='coerce')\n",
    "dat_wide_full['start_Time'] = pd.to_datetime(dat_wide_full['start_Time'] / 1000, unit='s')\n",
    "\n",
    "dat_wide_full['end_Time'] = pd.to_numeric(dat_wide_full['end_Time'], errors='coerce')\n",
    "dat_wide_full['end_Time'] = pd.to_datetime(dat_wide_full['end_Time'] / 1000, unit='s')\n",
    "\n",
    "dat_wide_full['schedule_Time'] = pd.to_numeric(dat_wide_full['schedule_Time'], errors='coerce')\n",
    "dat_wide_full['schedule_Time'] = pd.to_datetime(dat_wide_full['schedule_Time'] / 1000, unit='s')\n",
    "\n",
    "# Timestamp cleanup \n",
    "dat_wide_full['schedule_Time'] = pd.to_datetime(dat_wide_full['schedule_Time'])\n",
    "dat_wide_full['schedule_Time'] = np.where(dat_wide_full['offset']== '1', dat_wide_full['schedule_Time'],  dat_wide_full['schedule_Time'].dt.tz_localize('UTC').dt.tz_convert('America/New_York').dt.tz_localize(None)) \n",
    "\n",
    "dat_wide_full['start_Time'] = pd.to_datetime(dat_wide_full['end_Time'])\n",
    "dat_wide_full['start_Time'] = dat_wide_full['end_Time'].dt.floor('1s')\n",
    "dat_wide_full['start_Time'] = np.where(dat_wide_full['offset']== '1', dat_wide_full['end_Time'], dat_wide_full['start_Time'].dt.tz_localize('UTC').dt.tz_convert('America/New_York').dt.tz_localize(None))\n",
    "\n",
    "dat_wide_full['end_Time'] = pd.to_datetime(dat_wide_full['end_Time'])\n",
    "dat_wide_full['end_Time'] = dat_wide_full['end_Time'].dt.floor('1s')\n",
    "dat_wide_full['end_Time'] = np.where(dat_wide_full['offset']== '1', dat_wide_full['end_Time'], dat_wide_full['end_Time'].dt.tz_localize('UTC').dt.tz_convert('America/New_York').dt.tz_localize(None))\n",
    "\n",
    "# Creating Dates\n",
    "dat_wide_full['scheduled_Date'] = dat_wide_full['schedule_Time'].dt.date\n",
    "dat_wide_full['start_Date'] = dat_wide_full['start_Time'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting list of secret_user_id for userIds. This step could be excluded in future, once export file and schedule file have same secret_user_id\n",
    "ID_List = dat_wide_full[['userId', 'secret_user_id']].drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data: Flows and activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Separting the data by flows vs activities\n",
    "    act_dat_wide = dat_wide_full[dat_wide_full['is_activity']=='Y']\n",
    "    dat_wide_full = dat_wide_full[dat_wide_full['is_activity']=='N']\n",
    "    \n",
    "    # Split activity_flow column to grab the activity_id and activity_name\n",
    "    act_dat_wide[['activity_id_report', 'activity_name_report']] = act_dat_wide['activity_flow'].str.split(\"|\", expand=True)\n",
    "\n",
    "    # Dropping unnecessary columns\n",
    "    act_dat_wide = act_dat_wide.drop(columns=['activity_flow', 'is_activity'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow Submissions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow schedule history cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp formatting\n",
    "history['scheduled_Time_start'] = history['scheduled_date'] + ' ' + history['schedule_start_time']\n",
    "history['scheduled_Time_start'] = pd.to_datetime(history['scheduled_Time_start'])\n",
    "history['scheduled_Date2'] = history['scheduled_Time_start'].dt.date\n",
    "\n",
    "history['scheduled_Time_end'] = history['scheduled_date'] + ' ' + history['schedule_end_time']\n",
    "history['scheduled_Time_end'] = pd.to_datetime(history['scheduled_Time_end'])\n",
    "\n",
    "# Renaming columns\n",
    "history = history.rename(columns={'secret_user_id': 'history_sui'})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow submission (report.csv) & Flow schedule history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine history table data with wide data to get missing schedule rows\n",
    "conn = sqlite3.connect(':memory:') # Make the db in memory\n",
    "#write the tables\n",
    "history.to_sql('history', conn, index=False)\n",
    "dat_wide_full.to_sql('dat_wide_full', conn, index=False)\n",
    "\n",
    "qry = '''\n",
    "    select  \n",
    "        *\n",
    "    from\n",
    "        history full outer join dat_wide_full\n",
    "            on history.user_id = dat_wide_full.userId\n",
    "            and history.event_id = dat_wide_full.event_id_report\n",
    "            and history.flow_id = dat_wide_full.activity_flow\n",
    "            and strftime('%Y-%m-%d %H:%M:%S' , dat_wide_full.schedule_Time) between strftime('%Y-%m-%d %H:%M:%S' , history.scheduled_Time_start) and strftime('%Y-%m-%d %H:%M:%S', history.scheduled_Time_end) \n",
    "    '''\n",
    "dat_joined = pd.read_sql_query(qry, conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if joined correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if joined correctly\n",
    "## !! Should not be 0 !!\n",
    "\n",
    "len(dat_joined[(pd.notna(dat_joined['history_sui']))&(pd.notna(dat_joined['userId']))].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export and Check joined data, if necessary\n",
    "\n",
    "# dat_joined.to_csv(os.path.join(output_path, 'flow_joined_check.csv'), index=False, date_format='%Y-%m-%d %H:%M:%S', na_rep='NA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flow Items cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 0:\n",
    "    # Setting this for binary values\n",
    "    flow['value'] = 1 \n",
    "    flow.rename(columns = {'applet_version':'version'},inplace=True)\n",
    "    # Wide binary column build\n",
    "    flow_wide = pd.pivot_table(flow, index=['version', 'flow_id'], columns='activity_name', values='value', aggfunc='first')\n",
    "    flow_wide = flow_wide.fillna(0)\n",
    "    flow_wide = flow_wide.reset_index()\n",
    "else:\n",
    "    # Creating activities binary columns\n",
    "    flow_activities_report = act_dat_wide.copy()\n",
    "    flow_activities_history = act_history.copy()\n",
    "\n",
    "    flow_activities_report = flow_activities_report[['activity_id_report', 'activity_name_report', 'version' ]]\n",
    "    flow_activities_history = flow_activities_history[['activity_id', 'activity_name', 'applet_version' ]]\n",
    "\n",
    "    flow_activities_report.rename(columns = {'activity_id_report':'activity_id', \n",
    "                                            'activity_name_report': 'activity_name', \n",
    "                                            'version': 'applet_version'}\n",
    "                                ,inplace=True)\n",
    "\n",
    "\n",
    "    flow_activities_report['value'] = 0\n",
    "    flow_activities_history['value'] = 0\n",
    "\n",
    "    flow_activities_all = [flow_activities_report, flow_activities_history]\n",
    "    flow_activities = pd.concat(flow_activities_all, ignore_index=True)\n",
    "\n",
    "    # Dropping duplicates for join\n",
    "    flow_activities = flow_activities.drop_duplicates()\n",
    "\n",
    "    # Getting other binary columns from flow history to ensure all activities are listed\n",
    "    flows= flow.copy()\n",
    "    flows = flows[['flow_id', 'activity_name', 'applet_version']]\n",
    "    flows.rename(columns = {'applet_version':'version'},inplace=True)\n",
    "    flows['value_flow'] = 1\n",
    "\n",
    "    # Dropping duplicates for join\n",
    "    flows = flows.drop_duplicates()\n",
    "\n",
    "    #Joining the two binary column data together\n",
    "    flows_final = pd.merge(flows, flow_activities, how='outer', left_on=['version', 'flow_id'], right_on=['applet_version', 'activity_id'])\n",
    "\n",
    "    #Filling in data with the join so all information is in one column\n",
    "    flows_final['flow_id_dup'] = flows_final['flow_id']\n",
    "    flows_final['flow_id'] = np.where(flows_final['flow_id_dup'].isna(), flows_final['activity_id'], flows_final['flow_id_dup'])\n",
    "\n",
    "    flows_final['activity_name'] = flows_final['activity_name_x']\n",
    "    flows_final['activity_name'] = np.where(flows_final['activity_name_x'].isna(), flows_final['activity_name_y'], flows_final['activity_name_x'])\n",
    "\n",
    "    flows_final['version_dup'] = flows_final['version']\n",
    "    flows_final['version'] = np.where(flows_final['version_dup'].isna(), flows_final['applet_version'], flows_final['version_dup'])\n",
    "\n",
    "    flows_final['value_dup'] = flows_final['value']\n",
    "    flows_final['value'] = np.where(flows_final['value_flow'].isna(), flows_final['value_dup'], flows_final['value_flow'])\n",
    "\n",
    "    flows_final = flows_final[['flow_id', 'activity_name', 'version', 'value']]\n",
    "\n",
    "    #Making binary data wide to join and filling any NAs with 0\n",
    "    flow_wide = pd.pivot_table(flows_final, index=['version',  'flow_id'], columns='activity_name', values='value', aggfunc='first').reset_index()\n",
    "    flow_wide = flow_wide.fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joined Flows Data and Flow Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the version column such that binary values exist for null response rows\n",
    "dat_joined['version_report'] = dat_joined['version']\n",
    "dat_joined['version'] = np.where(dat_joined['version_report'].isna(), dat_joined['applet_version'], dat_joined['version_report'])\n",
    "\n",
    "# Getting the flow column such that binary values exist for null response rows\n",
    "dat_joined['activity_flow_id'] = np.where(dat_joined['activity_flow'].isna(), dat_joined['flow_id'], dat_joined['activity_flow']) \n",
    "dat_joined['activity_flow'] = np.where(dat_joined['activity_flow_name'].isna(), dat_joined['flow_name'], dat_joined['activity_flow_name']) \n",
    "\n",
    "# Merging Wide fomart export data with binary wide data\n",
    "dat_wide2 = pd.merge(dat_joined, flow_wide, how='left', left_on=['version', 'activity_flow_id'], right_on=['version', 'flow_id'])\n",
    "\n",
    "# Getting the user_id column to ensure user_id is always available\n",
    "dat_wide2['userId_report'] = dat_wide2['userId']\n",
    "dat_wide2['userId'] = np.where(dat_wide2['userId_report'].isna(), dat_wide2['user_id'], dat_wide2['userId_report'])\n",
    "\n",
    "# Combining Headache Time separated version and time range version together\n",
    "dat_wide2['headache_time_start_dup'] = dat_wide2['headache_time_start']\n",
    "dat_wide2['headache_time_start'] = np.where((dat_wide2['headache_time_start_dup']=='nan')|(dat_wide2['headache_time_start_dup'].isna()), dat_wide2['headache_time_start_old'], dat_wide2['headache_time_start_dup'])\n",
    "dat_wide2['headache_time_end_dup'] = dat_wide2['headache_time_end']\n",
    "dat_wide2['headache_time_end'] = np.where((dat_wide2['headache_time_end_dup']=='nan')|(dat_wide2['headache_time_end_dup'].isna()), dat_wide2['headache_time_end_old'], dat_wide2['headache_time_end_dup'])\n",
    "\n",
    "# Timestamp formatting - TO MAKE SURE!\n",
    "dat_wide2['schedule_Time'] = pd.to_datetime(dat_wide2['schedule_Time'])\n",
    "dat_wide2['start_Time'] = pd.to_datetime(dat_wide2['start_Time'])\n",
    "dat_wide2['end_Time'] = pd.to_datetime(dat_wide2['end_Time'])\n",
    "dat_wide2['scheduled_Time_start'] = pd.to_datetime(dat_wide2['scheduled_Time_start'])\n",
    "\n",
    "# Getting secret_user_id for all the user_ids. This step could be excluded in future, once export file and schedule file have same secret_user_id\n",
    "final = pd.merge(dat_wide2, ID_List, how='left', left_on=['userId'], right_on=['userId'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flows Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Ensure secret_user_id available for all rows\n",
    "final['secret_user_id'] = np.where(final['secret_user_id_x'].isna(), final['secret_user_id_y'], final['secret_user_id_x'])\n",
    "final['secret_user_id'] = np.where(final['secret_user_id'].isna(), final['history_sui'], final['secret_user_id'])\n",
    "\n",
    "# If activity schedule time is null, it will be replaced with timestamp from schedule file \n",
    "final['schedule_Time'] = np.where(final['schedule_Time'].isna(), final['scheduled_Time_start'], final['schedule_Time'])\n",
    "\n",
    "# If event_id is null, it will be replaced with event_id from schedule file \n",
    "final['event_id_sched'] = final['event_id']\n",
    "final['event_id'] = np.where(final['event_id_report'].isna(), final['event_id_sched'], final['event_id_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################### PLEASE SELECT APPROPRIATE ITEMS & ORDER ###############################################################\n",
    "\n",
    "# selecting the required columns and ordering \n",
    "\n",
    "final = final[[\n",
    "    'userId',\n",
    "    'event_id',\n",
    "    'id',\n",
    "    'secret_user_id',\n",
    "     'activity_flow',\n",
    "     'schedule_Time',\n",
    "     'start_Time',\n",
    "     'end_Time',\n",
    "     'version',\n",
    "    ' Sleep ',\n",
    "     'Activity Watch',\n",
    "     'Context of Assessment',\n",
    "     'Diet',\n",
    "    #  'Diet Test Activity schedule', #TESTING ACTIVITY\n",
    "     'Food and Drink Intake',\n",
    "     'Food and Drink Intake (Supplemental)', #Only in Testing\n",
    "     'Life Events',\n",
    "     'Light Device',\n",
    "     'Menstrual Period',\n",
    "     'Mood Circumplex',\n",
    "     'Pain',\n",
    "     'Physical Activity',\n",
    "     'Physical Activity (with Supplement)', #Only in Testing\n",
    "     'Physical Health',\n",
    "     'Procedure Tracking', #Only in Testing\n",
    "     'Saliva Sample',\n",
    "     'Substance Use', #Only in Testing\n",
    "     'Substance Use (AM)', #Only in Testing\n",
    "     'Substance Use (Core)', #Only in Testing\n",
    "     'Substance Use (Supplement)',\n",
    "    'saliva_instructions', \n",
    "    'since_activity_monitor',\n",
    "    'since_activity_monitor_time_start', \n",
    "    'since_activity_monitor_time_end', \n",
    "    'since_light_device',\n",
    "    'since_light_device_time_start', \n",
    "    'since_light_device_time_end', \n",
    "    'morning_bedtime',\n",
    "    'morning_lights_off',\n",
    "    'morning_fall_asleep',\n",
    "    'morning_wake_number',\n",
    "    'morning_awakenings_last',\n",
    "    'morning_waketime',\n",
    "    'morning_outbed',\n",
    "    'morning_sleep_quality',\n",
    "    'morning_sleep_refreshed',\n",
    "    'morning_sleep_problems',\n",
    "    'morning_sleeping_pills',\n",
    "    'morning_sleeping_pills_type',\n",
    "    'now_gps_lat', \n",
    "    'now_gps_long', \n",
    "    'now_where',\n",
    "    'now_inside',\n",
    "    'now_outside',\n",
    "    'since_where',\n",
    "    'since_inside',\n",
    "    'since_outside',\n",
    "    'now_company',\n",
    "    # 'now_company_confirm',\n",
    "    'since_company',\n",
    "    'now_activity',\n",
    "    'since_activity',\n",
    "    'since_internet',\n",
    "    'internet_use_duration',\n",
    "    'internet_use_category',\n",
    "    'videogame_duration',\n",
    "    'socialmedia_duration',\n",
    "    'socialmedia_activity',\n",
    "    'friends_communication_method',\n",
    "    'strangers_communication_method',\n",
    "    'comments_byothers',\n",
    "    'comments_byself',\n",
    "    'now_sadness',\n",
    "    'now_anxiousness',\n",
    "    'now_active',\n",
    "    'now_tired',\n",
    "    'now_distracted',\n",
    "    'now_irritable',\n",
    "    'now_quick_thinking',\n",
    "    'now_enjoyment',\n",
    "    'now_fidgety',\n",
    "    'now_thoughts_positive',\n",
    "    'now_thoughts_negative',\n",
    "    'now_thoughts_negative_about',\n",
    "    'now_thoughts_negative_severity',\n",
    "    'since_thoughts_negative',\n",
    "    'since_thoughts_negative_suicide',\n",
    "    'since_thoughts_negative_suicide_message',\n",
    "    'instructions', #event_instructions\n",
    "    'event_emotion',\n",
    "    'event_category',\n",
    "    'event_people',\n",
    "    'event_where',\n",
    "    'event_health',\n",
    "    'event_content',\n",
    "    'event_issue',\n",
    "    'event_stress',\n",
    "    'since_physical_activity',\n",
    "    'since_sedentary',\n",
    "    'since_nap_rest_time',\n",
    "    'since_rest_duration',\n",
    "    'since_rest_fell_asleep',\n",
    "    'since_sleep_duration',\n",
    "    'since_vigorous_activity',\n",
    "    'since_moderate_activity',\n",
    "    'since_light_activity',\n",
    "    'now_thirsty',\n",
    "    'since_had_drink',\n",
    "    'not_alcohol_amount',\n",
    "    'since_had_drink_alcohol_type',\n",
    "    'since_had_drink_alcohol_quantity',\n",
    "    'alcohol_time',\n",
    "    'since_feel_drink',\n",
    "    'since_had_drink_caffeinated_type',\n",
    "    'now_hungry',\n",
    "    'since_times_eat',\n",
    "    'since_eaten_amount',\n",
    "    'since_eaten_when',\n",
    "    'since_eaten_type',\n",
    "    'since_food1',\n",
    "    'since_food2',\n",
    "    'since_food3',\n",
    "    'since_food4',\n",
    "    'since_food5',\n",
    "    'since_crave',\n",
    "    'craving_strong_tobacco',\n",
    "    'craving_strong_cannabis',\n",
    "    'craving_strong_otherdrug',\n",
    "    'craving_strong_alcohol',\n",
    "    'since_substances',\n",
    "    'substances_tobacco',\n",
    "    'since_substances_cigarettes',\n",
    "    'since_substances_cigarettes_time',\n",
    "    'since_substances_enicotine',\n",
    "    'since_substances_enicotine_time',\n",
    "    'since_substances_other_tobacco',\n",
    "    'since_substances_other_tobacco_time',\n",
    "    'since_cannabis_type',\n",
    "    'since_cannabis_time',\n",
    "    'since_substances_cannabis',\n",
    "    'since_cannabis_high',\n",
    "    'since_substances_other',\n",
    "    'since_substances_other_specify',\n",
    "    'since_substances_other_time',\n",
    "    'since_substances_other_high',\n",
    "    'since_substances_company',\n",
    "    'now_pain',\n",
    "    'now_pain_where',\n",
    "    'since_pain',\n",
    "    'since_pain_where',\n",
    "    'pain_intensity',\n",
    "    'headache',\n",
    "    'headache_prevent',\n",
    "    'headache_time_start', \n",
    "    'headache_time_end', \n",
    "    'headache_intensity',\n",
    "    'headache_location',\n",
    "    'headache_pulsating',\n",
    "    'headache_effort',\n",
    "    'headache_nausea',\n",
    "    'headache_light',\n",
    "    'headache_heat',\n",
    "    'headache_noise',\n",
    "    'headache_smell',\n",
    "    'headache_trigger',\n",
    "    'headache_vision_changes',\n",
    "    'headache_vision_change_time',\n",
    "    'headache_numbing',\n",
    "    'headache_numbing_time',\n",
    "    'headache_confusing',\n",
    "    'headache_confusing_time',\n",
    "    'headache_sudden',\n",
    "    'headache_medication',\n",
    "    'headache_interference',\n",
    "    'day_physical_health',\n",
    "    'day_problem_categories',\n",
    "    'day_bother',\n",
    "    'day_over_medication',\n",
    "    'day_over_medication_why',\n",
    "    'day_prescribed_medication',\n",
    "    'day_prescribed_medication_conditions',\n",
    "    'day_problems_allergies',\n",
    "    'day_problems_breath',\n",
    "    'day_problems_belly_symptoms',\n",
    "    'day_problems_belly',\n",
    "    'day_problems_muscle',\n",
    "    'day_problems_heart',\n",
    "    'dizziness_situation',\n",
    "    'dizziness_faint',\n",
    "    'day_lethargic',\n",
    "    'activity_planned',\n",
    "    'day_period', \n",
    "    ##New items (2024 Jul 31st)\n",
    "    'audio_test', \n",
    "    'headache_current', \n",
    "    'headache_same', \n",
    "    'since_activity_monitor_now', \n",
    "    # 'since_cannabis_craving',\n",
    "    # 'since_light_activity_planned',\n",
    "    'since_light_device_now',\n",
    "    # 'since_moderate_activity_planned',\n",
    "    # 'since_substances_other_crave',\n",
    "    # 'since_vigorous_activity_planned',\n",
    "    # 'substances_lastnight_time',\n",
    "    # 'substances_lastnight'\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>event_id</th>\n",
       "      <th>id</th>\n",
       "      <th>secret_user_id</th>\n",
       "      <th>activity_flow</th>\n",
       "      <th>schedule_Time</th>\n",
       "      <th>start_Time</th>\n",
       "      <th>end_Time</th>\n",
       "      <th>version</th>\n",
       "      <th>Sleep</th>\n",
       "      <th>...</th>\n",
       "      <th>dizziness_situation</th>\n",
       "      <th>dizziness_faint</th>\n",
       "      <th>day_lethargic</th>\n",
       "      <th>activity_planned</th>\n",
       "      <th>day_period</th>\n",
       "      <th>audio_test</th>\n",
       "      <th>headache_current</th>\n",
       "      <th>headache_same</th>\n",
       "      <th>since_activity_monitor_now</th>\n",
       "      <th>since_light_device_now</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64e7a9b1-22d8-1858-d681-d6ef00000000</td>\n",
       "      <td>ba06582f-b106-4c27-8d6e-f8a424883c17</td>\n",
       "      <td>NA</td>\n",
       "      <td>[admin account] (11c95918-c364-4a56-82ff-1017a...</td>\n",
       "      <td>Mid-day Assessment</td>\n",
       "      <td>2024-01-11 16:00:00</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>35.0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6297aa0d-b90b-7f10-4d02-e84200000000</td>\n",
       "      <td>62b4499c-819d-45c3-ad1f-2c3a6bf633f4</td>\n",
       "      <td>50b28a9d-1ede-42c9-bd8e-76b074e83dd0|50b28a9d-...</td>\n",
       "      <td>[admin account] (3c185bfd-3418-4675-ae3d-1a635...</td>\n",
       "      <td>Evening Assessment</td>\n",
       "      <td>2024-01-12 23:00:00</td>\n",
       "      <td>2024-01-12 23:16:15</td>\n",
       "      <td>2024-01-12 23:16:15</td>\n",
       "      <td>29.2.12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>0</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6297aa0d-b90b-7f10-4d02-e84200000000</td>\n",
       "      <td>519c65a1-fda8-488a-8eea-8eb621411443</td>\n",
       "      <td>NA</td>\n",
       "      <td>[admin account] (3c185bfd-3418-4675-ae3d-1a635...</td>\n",
       "      <td>Mid-day Assessment</td>\n",
       "      <td>2024-01-12 12:30:00</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>35.0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6297aa0d-b90b-7f10-4d02-e84200000000</td>\n",
       "      <td>775713d1-738d-4d39-91f7-9826a90e50d3</td>\n",
       "      <td>NA</td>\n",
       "      <td>[admin account] (3c185bfd-3418-4675-ae3d-1a635...</td>\n",
       "      <td>Morning Assessment</td>\n",
       "      <td>2024-01-12 09:00:00</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>35.0.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64e7a9b1-22d8-1858-d681-d6ef00000000</td>\n",
       "      <td>45b7edf9-d57b-40fb-94c1-fd4c7e3bdb1d</td>\n",
       "      <td>NA</td>\n",
       "      <td>[admin account] (11c95918-c364-4a56-82ff-1017a...</td>\n",
       "      <td>Evening Assessment</td>\n",
       "      <td>2024-01-12 20:00:00</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>35.0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "      <td>NA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 userId                              event_id  \\\n",
       "0  64e7a9b1-22d8-1858-d681-d6ef00000000  ba06582f-b106-4c27-8d6e-f8a424883c17   \n",
       "1  6297aa0d-b90b-7f10-4d02-e84200000000  62b4499c-819d-45c3-ad1f-2c3a6bf633f4   \n",
       "2  6297aa0d-b90b-7f10-4d02-e84200000000  519c65a1-fda8-488a-8eea-8eb621411443   \n",
       "3  6297aa0d-b90b-7f10-4d02-e84200000000  775713d1-738d-4d39-91f7-9826a90e50d3   \n",
       "4  64e7a9b1-22d8-1858-d681-d6ef00000000  45b7edf9-d57b-40fb-94c1-fd4c7e3bdb1d   \n",
       "\n",
       "                                                  id  \\\n",
       "0                                                 NA   \n",
       "1  50b28a9d-1ede-42c9-bd8e-76b074e83dd0|50b28a9d-...   \n",
       "2                                                 NA   \n",
       "3                                                 NA   \n",
       "4                                                 NA   \n",
       "\n",
       "                                      secret_user_id       activity_flow  \\\n",
       "0  [admin account] (11c95918-c364-4a56-82ff-1017a...  Mid-day Assessment   \n",
       "1  [admin account] (3c185bfd-3418-4675-ae3d-1a635...  Evening Assessment   \n",
       "2  [admin account] (3c185bfd-3418-4675-ae3d-1a635...  Mid-day Assessment   \n",
       "3  [admin account] (3c185bfd-3418-4675-ae3d-1a635...  Morning Assessment   \n",
       "4  [admin account] (11c95918-c364-4a56-82ff-1017a...  Evening Assessment   \n",
       "\n",
       "         schedule_Time           start_Time             end_Time  version  \\\n",
       "0  2024-01-11 16:00:00                   NA                   NA  35.0.11   \n",
       "1  2024-01-12 23:00:00  2024-01-12 23:16:15  2024-01-12 23:16:15  29.2.12   \n",
       "2  2024-01-12 12:30:00                   NA                   NA  35.0.11   \n",
       "3  2024-01-12 09:00:00                   NA                   NA  35.0.11   \n",
       "4  2024-01-12 20:00:00                   NA                   NA  35.0.11   \n",
       "\n",
       "   Sleep   ... dizziness_situation dizziness_faint day_lethargic  \\\n",
       "0     0.0  ...                  NA              NA            NA   \n",
       "1     0.0  ...                   0               0             0   \n",
       "2     0.0  ...                  NA              NA            NA   \n",
       "3     1.0  ...                  NA              NA            NA   \n",
       "4     0.0  ...                  NA              NA            NA   \n",
       "\n",
       "  activity_planned day_period audio_test headache_current headache_same  \\\n",
       "0               NA         NA         NA               NA            NA   \n",
       "1               NA          0         NA               NA            NA   \n",
       "2               NA         NA         NA               NA            NA   \n",
       "3               NA         NA         NA               NA            NA   \n",
       "4               NA         NA         NA               NA            NA   \n",
       "\n",
       "  since_activity_monitor_now since_light_device_now  \n",
       "0                         NA                     NA  \n",
       "1                         NA                     NA  \n",
       "2                         NA                     NA  \n",
       "3                         NA                     NA  \n",
       "4                         NA                     NA  \n",
       "\n",
       "[5 rows x 195 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensuring consistent NA wording across all columns\n",
    "final_out = final.copy()\n",
    "final_out.replace('nan', 'NA', inplace=True)\n",
    "final_out.fillna('NA', inplace=True)\n",
    "final_out.replace('NA NA', 'NA', inplace=True)\n",
    "final_out.rename(columns = {'userId_x':'user_id'},inplace=True)\n",
    "\n",
    "# Excluding test flows\n",
    "final_out = final_out[final_out['activity_flow'] != 'Test Flow (All Activities)']\n",
    "\n",
    "# Checking final flow data\n",
    "final_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out final flow dataframe, if necessary:\n",
    "\n",
    "final_out.to_csv(os.path.join(output_path, 'flow_final.csv'), index=False, date_format='%Y-%m-%d %H:%M:%S', na_rep='NA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check ACTIVITIES Submissions data:\n",
    "# act_dat_wide.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity schedule history cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Timestamp formatting\n",
    "    act_history['scheduled_Time_start'] = act_history['scheduled_date'] + ' ' + act_history['schedule_start_time']\n",
    "    act_history['scheduled_Time_start'] = pd.to_datetime(act_history['scheduled_Time_start'])\n",
    "    act_history['scheduled_Date2'] = act_history['scheduled_Time_start'].dt.date\n",
    "\n",
    "    act_history['scheduled_Time_end'] = act_history['scheduled_date'] + ' ' + act_history['schedule_end_time']\n",
    "    act_history['scheduled_Time_end'] = pd.to_datetime(act_history['scheduled_Time_end'])\n",
    "    \n",
    "    # Renaming columns\n",
    "    act_history = act_history.rename(columns={'secret_user_id': 'history_sui'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity Submission (report.csv) & Activity schedule history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Combine history table data with wide data to get missing schedule rows\n",
    "    conn = sqlite3.connect(':memory:') # Make the db in memory\n",
    "    #write the tables\n",
    "    act_history.to_sql('history', conn, index=False)\n",
    "    act_dat_wide.to_sql('dat_wide_full', conn, index=False)\n",
    "\n",
    "    qry = '''\n",
    "        select  \n",
    "            *\n",
    "        from\n",
    "            history full outer join dat_wide_full\n",
    "            on history.user_id = dat_wide_full.userId\n",
    "            and history.event_id = dat_wide_full.event_id_report\n",
    "            and history.activity_id = dat_wide_full.activity_id_report\n",
    "            and strftime('%Y-%m-%d %H:%M:%S' , dat_wide_full.schedule_Time) between strftime('%Y-%m-%d %H:%M:%S' , history.scheduled_Time_start) and strftime('%Y-%m-%d %H:%M:%S', history.scheduled_Time_end) \n",
    "        '''\n",
    "    act_dat_joined = pd.read_sql_query(qry, conn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if it joined correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# Check if joined correctly\n",
    "## !! Should not be 0 !!\n",
    "if act_history_exist == 1:\n",
    "    print(len(act_dat_joined[(pd.notna(act_dat_joined['history_sui']))&(pd.notna(act_dat_joined['userId']))].index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export and Check joined data, if necessary\n",
    "\n",
    "# if act_history_exist == 1:\n",
    "#     dat_joined.to_csv(os.path.join(output_path, 'activities_joined_check.csv'), index=False, date_format='%Y-%m-%d %H:%M:%S', na_rep='NA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joined Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Getting the user_id column to ensure user_id is always available\n",
    "    act_dat_joined['userId_report'] = act_dat_joined['userId']\n",
    "    act_dat_joined['userId'] = np.where(act_dat_joined['userId_report'].isna(), act_dat_joined['user_id'], act_dat_joined['userId_report'])\n",
    "\n",
    "\n",
    "    # Getting the version column such that binary values exist for null response rows\n",
    "    act_dat_joined['version_report'] = act_dat_joined['version']\n",
    "    act_dat_joined['version'] = np.where(act_dat_joined['version_report'].isna(), act_dat_joined['applet_version'], act_dat_joined['version_report'])\n",
    "\n",
    "    # If event_id is null, it will be replaced with event_id from schedule file \n",
    "    act_dat_joined['event_id_sched'] = act_dat_joined['event_id']\n",
    "    act_dat_joined['event_id'] = np.where(act_dat_joined['event_id_report'].isna(), act_dat_joined['event_id_sched'], act_dat_joined['event_id_report'])\n",
    "\n",
    "    #  Ensure secret_user_id available for all rows\n",
    "    act_dat_joined['secret_user_id'] = np.where(act_dat_joined['secret_user_id'].isna(), act_dat_joined['history_sui'], act_dat_joined['secret_user_id'])\n",
    "\n",
    "    #  Ensure activity_name available for all rows\n",
    "    act_dat_joined['activity_name_history'] = act_dat_joined['activity_name']\n",
    "    act_dat_joined['activity_name'] = np.where(act_dat_joined['activity_name_report'].isna(), act_dat_joined['activity_name_history'], act_dat_joined['activity_name_report'])\n",
    "\n",
    "    #  Ensure activity_id available for all rows\n",
    "    act_dat_joined['activity_id_history'] = act_dat_joined['activity_id']\n",
    "    act_dat_joined['activity_id'] = np.where(act_dat_joined['activity_id_report'].isna(), act_dat_joined['activity_id_history'], act_dat_joined['activity_id_report'])\n",
    "\n",
    "    # Combining Headache Time separated version and time range version together\n",
    "    act_dat_joined['headache_time_start_dup'] = act_dat_joined['headache_time_start']\n",
    "    act_dat_joined['headache_time_start'] = np.where((act_dat_joined['headache_time_start_dup']=='nan')|(act_dat_joined['headache_time_start_dup'].isna()), act_dat_joined['headache_time_start_old'], act_dat_joined['headache_time_start_dup'])\n",
    "    act_dat_joined['headache_time_end_dup'] = act_dat_joined['headache_time_end']\n",
    "    act_dat_joined['headache_time_end'] = np.where((act_dat_joined['headache_time_end_dup']=='nan')|(act_dat_joined['headache_time_end_dup'].isna()), act_dat_joined['headache_time_end_old'], act_dat_joined['headache_time_end_dup'])\n",
    "\n",
    "    # Timestamp formatting - TO MAKE SURE!\n",
    "    act_dat_joined['schedule_Time'] = pd.to_datetime(act_dat_joined['schedule_Time'])\n",
    "    act_dat_joined['start_Time'] = pd.to_datetime(act_dat_joined['start_Time'])\n",
    "    act_dat_joined['end_Time'] = pd.to_datetime(act_dat_joined['end_Time'])\n",
    "    act_dat_joined['scheduled_Time_start'] = pd.to_datetime(act_dat_joined['scheduled_Time_start'])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity Name Binary Column Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d5/xkxxzbjs3l3923fx4_gbfjs40000gq/T/ipykernel_12542/1164555626.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  other_activities['value_flow'] = 0\n"
     ]
    }
   ],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Creating activities binary columns\n",
    "    activities = act_dat_joined.copy()\n",
    "    activities = activities[['activity_id', 'activity_name', 'version' ]]\n",
    "    activities['value'] = 1\n",
    "\n",
    "    # Dropping duplicates for join\n",
    "    activities = activities.drop_duplicates()\n",
    "\n",
    "    # Getting other binary columns from flow history to ensure all activities are listed\n",
    "    other_activities = flow.copy()\n",
    "    other_activities = flow[['activity_id', 'activity_name', 'applet_version']]\n",
    "    other_activities['value_flow'] = 0\n",
    "\n",
    "    # Dropping duplicates for join\n",
    "    other_activities = other_activities.drop_duplicates()\n",
    "\n",
    "    #Joining the two binary column data together\n",
    "    activities_final = pd.merge(other_activities, activities, how='outer', left_on=['applet_version', 'activity_id'], right_on=['version', 'activity_id'])\n",
    "\n",
    "    #Filling in data with the join so all information is in one column\n",
    "    activities_final['values'] = np.where(activities_final['value'].isna(), activities_final['value_flow'], activities_final['value'])\n",
    "    activities_final['activity_name'] = np.where(activities_final['activity_name_x'].isna(), activities_final['activity_name_y'], activities_final['activity_name_x'])\n",
    "    activities_final['version_activity'] = activities_final['version']\n",
    "    activities_final['version'] = np.where(activities_final['applet_version'].isna(), activities_final['version_activity'], activities_final['applet_version'])\n",
    "    activities_final = activities_final[['activity_id', 'activity_name', 'version', 'values' ]]\n",
    "\n",
    "    #Making binary data wide to join and filling any NAs with 0\n",
    "    activities_wide = pd.pivot_table(activities_final, index=['version',  'activity_id', 'activity_name', ], columns='activity_name', values='values', aggfunc='first').reset_index()\n",
    "    activities_wide = activities_wide.fillna(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joined Data and Activities Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Merging Wide fomart export data with binary wide data\n",
    "    act_final = pd.merge(act_dat_joined, activities_wide, how='left', left_on=['version', 'activity_id'], right_on=['version', 'activity_id'])\n",
    "\n",
    "    # Ensuring schedule time shows up\n",
    "    act_final['schedule_Time'] = np.where(act_final['schedule_Time'].isna(), act_final['scheduled_Time_start'], act_final['schedule_Time'])\n",
    "\n",
    "    # Final Timestamp formatting\n",
    "    act_final['schedule_Time'] = pd.to_datetime(act_final['schedule_Time'])\n",
    "    act_final['start_Time'] = pd.to_datetime(act_final['start_Time'])\n",
    "    act_final['end_Time'] = pd.to_datetime(act_final['end_Time'])\n",
    "\n",
    "    #Creating activity_flow column to match the flow final data\n",
    "    act_final['activity_flow'] = np.nan\n",
    "\n",
    "    #Check to make sure you see all activity names\n",
    "    list(activities_wide.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activities Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################### PLEASE SELECT APPROPRIATE ITEMS & ORDER ############################################################################\n",
    "\n",
    "# selecting the required columns and ordering \n",
    "if act_history_exist == 1:\n",
    "    act_final = act_final[[\n",
    "        'userId',\n",
    "        'event_id',\n",
    "        'id',\n",
    "        'secret_user_id',\n",
    "        'activity_flow',\n",
    "        'schedule_Time',\n",
    "        'start_Time',\n",
    "        'end_Time',\n",
    "        'version',\n",
    "        ' Sleep ',\n",
    "        'Activity Watch',\n",
    "        'Context of Assessment',\n",
    "        'Diet',\n",
    "        'Diet Test Activity schedule', #TESTING ACTIVITY\n",
    "        'Food and Drink Intake',\n",
    "        'Food and Drink Intake (Supplemental)', #Only in Testing\n",
    "        'Life Events',\n",
    "        'Light Device',\n",
    "        'Menstrual Period',\n",
    "        'Mood Circumplex',\n",
    "        'Pain',\n",
    "        'Physical Activity',\n",
    "        'Physical Activity (with Supplement)', #Only in Testing\n",
    "        'Physical Health',\n",
    "        'Procedure Tracking', #Only in Testing\n",
    "        'Saliva Sample',\n",
    "        'Substance Use', #Only in Testing\n",
    "        'Substance Use (AM)', #Only in Testing\n",
    "        'Substance Use (Core)', #Only in Testing\n",
    "        'Substance Use (Supplement)',\n",
    "        'saliva_instructions', \n",
    "        'since_activity_monitor',\n",
    "        'since_activity_monitor_time_start', \n",
    "        'since_activity_monitor_time_end', \n",
    "        'since_light_device',\n",
    "        'since_light_device_time_start', \n",
    "        'since_light_device_time_end', \n",
    "        'morning_bedtime',\n",
    "        'morning_lights_off',\n",
    "        'morning_fall_asleep',\n",
    "        'morning_wake_number',\n",
    "        'morning_awakenings_last',\n",
    "        'morning_waketime',\n",
    "        'morning_outbed',\n",
    "        'morning_sleep_quality',\n",
    "        'morning_sleep_refreshed',\n",
    "        'morning_sleep_problems',\n",
    "        'morning_sleeping_pills',\n",
    "        'morning_sleeping_pills_type',\n",
    "        'now_gps_lat', \n",
    "        'now_gps_long', \n",
    "        'now_where',\n",
    "        'now_inside',\n",
    "        'now_outside',\n",
    "        'since_where',\n",
    "        'since_inside',\n",
    "        'since_outside',\n",
    "        'now_company',\n",
    "        # 'now_company_confirm',\n",
    "        'since_company',\n",
    "        'now_activity',\n",
    "        'since_activity',\n",
    "        'since_internet',\n",
    "        'internet_use_duration',\n",
    "        'internet_use_category',\n",
    "        'videogame_duration',\n",
    "        'socialmedia_duration',\n",
    "        'socialmedia_activity',\n",
    "        'friends_communication_method',\n",
    "        'strangers_communication_method',\n",
    "        'comments_byothers',\n",
    "        'comments_byself',\n",
    "        'now_sadness',\n",
    "        'now_anxiousness',\n",
    "        'now_active',\n",
    "        'now_tired',\n",
    "        'now_distracted',\n",
    "        'now_irritable',\n",
    "        'now_quick_thinking',\n",
    "        'now_enjoyment',\n",
    "        'now_fidgety',\n",
    "        'now_thoughts_positive',\n",
    "        'now_thoughts_negative',\n",
    "        'now_thoughts_negative_about',\n",
    "        'now_thoughts_negative_severity',\n",
    "        'since_thoughts_negative',\n",
    "        'since_thoughts_negative_suicide',\n",
    "        'since_thoughts_negative_suicide_message',\n",
    "        'instructions', #event_instructions\n",
    "        'event_emotion',\n",
    "        'event_category',\n",
    "        'event_people',\n",
    "        'event_where',\n",
    "        'event_health',\n",
    "        'event_content',\n",
    "        'event_issue',\n",
    "        'event_stress',\n",
    "        'since_physical_activity',\n",
    "        'since_sedentary',\n",
    "        'since_nap_rest_time',\n",
    "        'since_rest_duration',\n",
    "        'since_rest_fell_asleep',\n",
    "        'since_sleep_duration',\n",
    "        'since_vigorous_activity',\n",
    "        'since_moderate_activity',\n",
    "        'since_light_activity',\n",
    "        'now_thirsty',\n",
    "        'since_had_drink',\n",
    "        'not_alcohol_amount',\n",
    "        'since_had_drink_alcohol_type',\n",
    "        'since_had_drink_alcohol_quantity',\n",
    "        'alcohol_time',\n",
    "        'since_feel_drink',\n",
    "        'since_had_drink_caffeinated_type',\n",
    "        'now_hungry',\n",
    "        'since_times_eat',\n",
    "        'since_eaten_amount',\n",
    "        'since_eaten_when',\n",
    "        'since_eaten_type',\n",
    "        'since_food1',\n",
    "        'since_food2',\n",
    "        'since_food3',\n",
    "        'since_food4',\n",
    "        'since_food5',\n",
    "        'since_crave',\n",
    "        'craving_strong_tobacco',\n",
    "        'craving_strong_cannabis',\n",
    "        'craving_strong_otherdrug',\n",
    "        'craving_strong_alcohol',\n",
    "        'since_substances',\n",
    "        'substances_tobacco',\n",
    "        'since_substances_cigarettes',\n",
    "        'since_substances_cigarettes_time',\n",
    "        'since_substances_enicotine',\n",
    "        'since_substances_enicotine_time',\n",
    "        'since_substances_other_tobacco',\n",
    "        'since_substances_other_tobacco_time',\n",
    "        'since_cannabis_type',\n",
    "        'since_cannabis_time',\n",
    "        'since_substances_cannabis',\n",
    "        'since_cannabis_high',\n",
    "        'since_substances_other',\n",
    "        'since_substances_other_specify',\n",
    "        'since_substances_other_time',\n",
    "        'since_substances_other_high',\n",
    "        'since_substances_company',\n",
    "        'now_pain',\n",
    "        'now_pain_where',\n",
    "        'since_pain',\n",
    "        'since_pain_where',\n",
    "        'pain_intensity',\n",
    "        'headache',\n",
    "        'headache_prevent',\n",
    "        'headache_time_start', \n",
    "        'headache_time_end', \n",
    "        'headache_intensity',\n",
    "        'headache_location',\n",
    "        'headache_pulsating',\n",
    "        'headache_effort',\n",
    "        'headache_nausea',\n",
    "        'headache_light',\n",
    "        'headache_heat',\n",
    "        'headache_noise',\n",
    "        'headache_smell',\n",
    "        'headache_trigger',\n",
    "        'headache_vision_changes',\n",
    "        'headache_vision_change_time',\n",
    "        'headache_numbing',\n",
    "        'headache_numbing_time',\n",
    "        'headache_confusing',\n",
    "        'headache_confusing_time',\n",
    "        'headache_sudden',\n",
    "        'headache_medication',\n",
    "        'headache_interference',\n",
    "        'day_physical_health',\n",
    "        'day_problem_categories',\n",
    "        'day_bother',\n",
    "        'day_over_medication',\n",
    "        'day_over_medication_why',\n",
    "        'day_prescribed_medication',\n",
    "        'day_prescribed_medication_conditions',\n",
    "        'day_problems_allergies',\n",
    "        'day_problems_breath',\n",
    "        'day_problems_belly_symptoms',\n",
    "        'day_problems_belly',\n",
    "        'day_problems_muscle',\n",
    "        'day_problems_heart',\n",
    "        'dizziness_situation',\n",
    "        'dizziness_faint',\n",
    "        'day_lethargic',\n",
    "        'activity_planned',\n",
    "        'day_period', \n",
    "        ##New items (2024 Jul 31st)\n",
    "        'audio_test', \n",
    "        'headache_current', \n",
    "        'headache_same', \n",
    "        'since_activity_monitor_now', \n",
    "        # 'since_cannabis_craving',\n",
    "        # 'since_light_activity_planned',\n",
    "        'since_light_device_now',\n",
    "        # 'since_moderate_activity_planned',\n",
    "        # 'since_substances_other_crave',\n",
    "        # 'since_vigorous_activity_planned',\n",
    "        # 'substances_lastnight_time',\n",
    "        # 'substances_lastnight'\n",
    "    ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    #Ensuring all NAs apper \"NA\"\n",
    "    act_final_out = act_final.copy()\n",
    "    act_final_out.replace('nan', 'NA', inplace=True)\n",
    "    act_final_out.fillna('NA', inplace=True)\n",
    "    act_final_out.replace('NA NA', 'NA', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see activities final output looks good.\n",
    "if act_history_exist == 1:\n",
    "    act_final_out.to_csv(os.path.join(output_path, 'activity_final.csv'), index=False, date_format='%Y-%m-%d %H:%M:%S', na_rep='NA')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Joining flow flow and activity final togeter \n",
    "    all_final = [final_out, act_final_out]\n",
    "    ema_out = pd.concat(all_final, ignore_index=True)\n",
    "\n",
    "    # Included similar to R script to ensure alignment with formatting\n",
    "    ema_out = ema_out.applymap(str)\n",
    "\n",
    "    #Final data output sorting\n",
    "    ema_out = ema_out.sort_values(by=['secret_user_id', 'schedule_Time'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check final output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "if act_history_exist == 1:\n",
    "    # checking for duplicates in final file. Ensure this always returns \"False\"\n",
    "    print(ema_out.duplicated().any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Preview final output\n",
    "    ema_out.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "if act_history_exist == 1:\n",
    "    # Write the final output file to csv\n",
    "\n",
    "    ema_out.to_csv(os.path.join(output_path, 'ema_output_final_v2.csv'), index=False, date_format='%Y-%m-%d %H:%M:%S', na_rep='NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
